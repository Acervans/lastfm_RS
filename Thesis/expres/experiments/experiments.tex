The experiments were designed to explore various sets of features for recommendation models, thoroughly testing each model with different combinations of features in order to identify the most effective configuration. The training process involved cross-validation with sets for training, validation, and testing. After each training epoch, validation was conducted, and if the validation scores did not improve within a specified number of epochs (mostly 10), the training would conclude; after which the models were evaluated on the testing set. Additionally, negative sampling was employed to help models distinguish relevant items from irrelevant ones.

Two evaluation methods were performed: labeled evaluation for explicit feedback, considering only positive or negative items by a rating threshold, which did not provide enough information to judge the recommendations from; and unlabeled evaluation for implicit feedback, which included ratings and treated seen items as positive. Specifically for the latter, the ``uni100'' evaluation mode sampled 100 negative items uniformly for each positive item in the testing set and then assessed the model's performance. Another implicit feedback mode, ``full'', was considered, which used the entire dataset for evaluation, but its time-consuming nature made it less practical. Finally, a seed was preset to ensure identical data for all models, and the applied evaluation metrics include \acs{ndcg}, Recall, Precision, \acs{map}, and \acs{mrr}, cut off at the 20 first items, with \acs{ndcg}@20 being the valid metric for early training stopping.